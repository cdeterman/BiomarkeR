% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/fs.stability.v2.R
\name{fs.stability}
\alias{fs.stability}
\title{Classification & Feature Selection}
\usage{
fs.stability(X, Y, method, k = 10, p = 0.9, f = ceiling(ncol(X)/10),
  stability.metric = "jaccard", optimize = TRUE,
  optimize.resample = FALSE, tuning.grid = NULL, k.folds = if (optimize)
  10 else NULL, repeats = if (k.folds == "LOO") NULL else if (optimize) 3 else
  NULL, resolution = if (is.null(tuning.grid) && optimize) 3 else NULL,
  metric = "Accuracy", model.features = FALSE, allowParallel = FALSE,
  verbose = FALSE, ...)
}
\arguments{
\item{X}{A scaled matrix or dataframe containing numeric values of
each feature}

\item{Y}{A factor vector containing group membership of samples}

\item{method}{A vector listing models to be fit.
Available options are \code{"plsda"} (Partial Least Squares
Discriminant Analysis), \code{"rf"} (Random Forest), \code{"gbm"}
(Gradient Boosting Machine), \code{"svm"} (Support Vector Machines),
\code{"glmnet"} (Elastic-net Generalized Linear Model),
 and \code{"pam"} (Prediction Analysis of Microarrays)}

\item{k}{Number of bootstrapped interations}

\item{p}{Percent of data to by 'trained'}

\item{f}{Number of features desired.  Default is top 10%
\code{"f = ceiling(ncol(X)/10)"}.
If rank correlation is desired, set \code{"f = NULL"}}

\item{stability.metric}{string indicating the type of stability metric.
Avialable options are \code{"jaccard"} (Jaccard Index/Tanimoto Distance),
 \code{"sorensen"} (Dice-Sorensen's Index), \code{"ochiai"} (Ochiai's Index),
 \code{"pof"} (Percent of Overlapping Features), \code{"kuncheva"}
 (Kuncheva's Stability Measures), \code{"spearman"} (Spearman Rank
 Correlation), and \code{"canberra"} (Canberra Distance)}

\item{optimize}{Logical argument determining if each model should
be optimized. Default \code{"optimize = TRUE"}}

\item{optimize.resample}{Logical argument determining if each resample
should be re-optimized. Default \code{"optimize.resample = FALSE"} - Only
one optimization run, subsequent models use initially determined parameters}

\item{tuning.grid}{Optional list of grids containing parameters to optimize
for each algorithm.  Default \code{"tuning.grid = NULL"} lets function
create grid determined by \code{"res"}}

\item{k.folds}{Number of folds generated during cross-validation.
May optionally be set to \code{"LOO"} for leave-one-out cross-validation.
Default \code{"k.folds = 10"}}

\item{repeats}{Number of times cross-validation repeated.
Default \code{"repeats = 3"}}

\item{resolution}{Resolution of model optimization grid.
Default \code{"resolution = 3"}}

\item{metric}{Criteria for model optimization.  Available options
are \code{"Accuracy"} (Predication Accuracy), \code{"Kappa"}
(Kappa Statistic), and \code{"AUC-ROC"}
(Area Under the Curve - Receiver Operator Curve)}

\item{model.features}{Logical argument if should have number of
features selected to be determined by the individual model runs.
Default \code{"model.features = FALSE"}}

\item{allowParallel}{Logical argument dictating if parallel processing
is allowed via foreach package. Default \code{allowParallel = FALSE}}

\item{verbose}{Logical argument if should output progress}

\item{...}{Extra arguments that the user would like to apply to the models}
}
\value{
\item{Methods}{Vector of models fit to data}

\item{performance}{Performance metrics of each model and
bootstrap iteration}

\item{RPT}{Robustness-Performance Trade-Off as defined in
Saeys 2008}

\item{features}{List concerning features determined via each
algorithms feature selection criteria.}

\itemize{
 \item{metric: Stability metric applied}
 \item{features: Matrix of selected features}
 \item{stability: Matrix of pairwise comparions and average stability}
 }

\item{stability.models}{Function perturbation metric - i.e. how
similar are the features selected
by each model.}

\item{original.best.tunes}{If \code{"optimize.resample = TRUE"}
then returns list of optimized parameters for each bootstrap.}

\item{final.best.tunes}{If \code{"optimize.resample = TRUE"}
then returns list of optimized parameters for each bootstrap of models
refit to selected features.}

\item{specs}{List with the following elements:}

\itemize{
 \item{total.samples: Number of samples in original dataset}
 \item{number.features: Number of features in orginal dataset}
 \item{number.groups: Number of groups}
 \item{group.levels: The specific levels of the groups}
 \item{number.observations.group: Number of observations in each group}}
}
\description{
Applies models to high-dimensional data to both classify and
determine important features for classification.  The function bootstraps
a user-specified number of times to facilitate stability metrics of
features selected thereby providing an important metric for biomarker
investigations, namely whether the important variables can be identified if
the models are refit on 'different' data.
}
\examples{
dat.discr <- create.discr.matrix(
    create.corr.matrix(
        create.random.matrix(nvar = 50, 
                             nsamp = 100, 
                             st.dev = 1, 
                             perturb = 0.2)),
    D = 10
)$discr.mat


vars <- dat.discr[,1:(ncol(dat.discr)-1)]
groups <- as.factor(dat.discr[,ncol(dat.discr)])

fits <- fs.stability(vars, 
                     groups, 
                     method = c("plsda", "rf"), 
                     f = 10, 
                     k = 3, 
                     k.folds = 10, 
                     verbose = FALSE)
}
\author{
Charles Determan Jr
}
\references{
Saeys Y., Abeel T., et. al. (2008) \emph{Machine Learning and
Knowledge Discovery in Databases}. 313-325.
http://link.springer.com/chapter/10.1007/978-3-540-87481-2_21
}

